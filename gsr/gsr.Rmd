---
title: "PRIMED example notebook: Genomic Summary Results"
output: html_notebook
---

# Import data to workspace: GSR example

We have uploaded GIANT Genomic Summary Results (GSR) to this workspace. In this notebook, we prepare the metadata about the analysis for import into an AnVIL workspace.

## Install and load R packages

```{r}
# get the latest version of AnVIL from github
#remotes::install_github("Bioconductor/AnVIL", dependencies=FALSE)
#remotes::install_github("UW-GAC/AnvilDataModels")
library(AnVIL)
library(AnvilDataModels)
library(tidyverse)
```

## Prepare tables according to data model

Metadata describing the analysis is stored in the analysis table. We save this as a set of "field" and "value" pairs for input to the workflow that assigns a unique identifier for each analysis.

```{r}
fields <- list(
    gsr_source = "https://portals.broadinstitute.org/collaboration/giant/index.php/GIANT_consortium_data_files",
    pubmed_id = "29273807",
    first_author = "V Turcot",
    link = "https://doi.org/10.1038%2Fs41588-017-0011-x",
    consent_code = "NRES",
    upload_date = "2022-07-27",
    contributor_contact = "sdmorris@uw.edu",
    trait = "BMI",
    trait_type = "quantitative",
    trait_unit = "kg / m^2",
    trait_transformation = "inverse normal",
    trait_definition = "weight / height",
    covariates = "age | age^2 | ancestry PCs",
    concept_id = "3038553",
    reference_assembly = "GRCh37",
    n_variants = "246328",
    genotyping_technology = "exome array",
    genotyping_platform = "Illumina",
    is_imputed = "FALSE",
    n_samples = "526508",
    eff_sample_size = "526508",
    age_min = "18",
    cohorts = "GIANT",
    is_meta_analysis = "TRUE",
    population_descriptors = "European | South Asian | African | East Asian | Hispanic",
    population_proportions = "85 | 6 | 5 | 2 | 2",
    countries_of_recruitment = "Australia | Bangladesh | China | Denmark | Estonia | Finnland | Germany | Greece | Iceland | Ireland | Italy | Netherlands | Norway | Philippines | Sweden | Taiwan | UK | USA",
    analysis_method = "score-statistics-based association analysis",
    analysis_software = "RAREMETALWORKER | RVTEST"
)
analysis <- tibble(field=names(fields),
                   value=unlist(fields))
```


Files are linked to analyses. The md5 hash of each file is used to generate the primary key for the 'file' table. The md5 should be computed before uploading to the workspace and checked after upload to make sure the upload was successful. 

```{r}
bucket <- avbucket()
bmi_file <- "BMI_All_ancestry.txt"
file_table <- tibble(md5sum = "ce323c092c3a6b4136ef53a95f6dc5fc",
                     file_path = file.path(bucket, bmi_file),
                     file_type = "data",
                     n_variants = 246328,
                     chromosome = "ALL")
```

The GIANT data is distributed as a single file per analysis. We read in the data, update the column names so it conforms to the PRIMED data model, and write each chromosome to a separate file.

```{r}
gsutil_cp(file.path(bucket, bmi_file), ".")
```

```{r}
dat <- read_tsv(bmi_file)
head(dat)
```

```{r}
dat2 <- dat %>%
  rename(chromosome = CHR,
         position = POS,
         rsID = SNPNAME,
         ref_allele = REF,
         alt_allele = ALT,
         p_value = Pvalue) %>%
  mutate(strand = NA,
         effect_allele = ref_allele,
         other_allele = alt_allele,
         effect_allele_freq = NA,
         n_samp = NA,
         is_imputed = FALSE)
```

```{r}
chrs <- unique(dat2$chromosome)
chr_file <- lapply(chrs, function(c) {
    # select only this chromosome
    dat_chr <- filter(dat2, chromosome == c)
    chr_file <- paste0("BMI_All_ancestry_chr", c, ".txt")
    
    # write tsv file
    write_tsv(dat_chr, file=chr_file, na="")
    
    # get md5sum for file table
    md5 <- tools::md5sum(chr_file)
    
    # copy file to google bucket
    gsutil_cp(chr_file, bucket)
    
    ## return row for file table
    tibble(
        md5sum = md5,
        n_variants = nrow(dat_chr),
        file_path = file.path(bucket, chr_file),
        file_type = "data",
        chromosome = c
    )
})
```

```{r}
file_tsv <- bind_rows(file_table, chr_file)
```


## Write tables as files to workspace bucket

To check the tables using a workflow, they must be written as files to the workspace bucket.

```{r}
outfile <- "GIANT_BMI_file_table.tsv"
write_tsv(file_tsv, outfile)
gsutil_cp(outfile, bucket)

outfile <- "GIANT_BMI_analysis_table.tsv"
write_tsv(analysis, outfile)
gsutil_cp(outfile, bucket)
```


## Check tables against data model

Once all tables have been created, we can check that they conform to the data model. This is most easily accomplished by providing the paths to the tables in TSV format as input to the `gsr_report` workflow.

## Check data files against data dictionary

Data files can be checked against the data dictionary with the `gsr_data_report` workflow.
