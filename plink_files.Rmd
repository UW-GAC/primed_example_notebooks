---
title: "PRIMED example notebook: PLINK files"
output: html_notebook
---

# Import data to workspace: PLINK example

We have uploaded HapMap data in PLINK format (bed/bim/fam) to this workspace. In this notebook, we import the data from these files into data tables.

## Install and load R packages

```{r}
# get the latest version of AnVIL from github
#remotes::install_github("Bioconductor/AnVIL", dependencies=FALSE)
#remotes::install_github("UW-GAC/AnvilDataModels")
library(AnVIL)
library(AnvilDataModels)
library(tidyverse)
```

## Files in workspace

To see what files are available in this workspace, we need the google bucket ID, which is returned by the `avbucket` function.

```{r}
(bucket <- avbucket())
gsutil_ls(bucket)
```

To read from a google bucket, we use the `gsutil_pipe` function. We need to specify `"rb"` as "read from binary".

```{r}
prefix <- "hapmap3_r3_b37_fwd.consensus.qc.poly_Ilmn1M"
famfile <- paste0(bucket, "/", prefix, ".fam")
famfile %in% gsutil_ls(bucket)
fampipe <- gsutil_pipe(famfile, "rb")
fam <- read_table(fampipe, col_names=c("family", "indiv", "father", "mother", "sex", "phen"), col_types="cccccc")
head(fam)
```

## Prepare tables according to data model

### Sample and subject tables

Create subject table

```{r}
subject <- fam %>%
    mutate(reported_sex=c("1"="M", "2"="F")[sex]) %>%
    select(subject_id=indiv, reported_sex) %>%
    mutate(consent_code="NRUP",
          study_nickname="HapMap",
          dbgap_submission=FALSE)
head(subject)
```

Create sample table. In this example we use the same identifiers for subject and sample, but different values for each are preferred.

```{r}
sample <- fam %>%
    select(sample_id=indiv) %>%
    mutate(subject_id=sample_id,
           tissue_source="cell line")
head(sample)
```


### Sample sets

Define sample set to link to genotype data. We will create two sets, one with all samples (recommended for inclusion in every workspace), and one with 100 samples that we will call "set1".

```{r}
sample_set <- create_set_all(sample, table_name="sample")
sample_set_100 <- tibble(sample_set_id="set1", sample_id=sample$sample_id[1:100])
sample_set <- bind_rows(sample_set, sample_set_100)
head(sample_set)
tail(sample_set)
count(sample_set, sample_set_id)
```

### Datasets

Each dataset is linked to a sample_set, but the same sample set may correspond to multiple datasets (such as array data and imputed data).

```{r}
dataset <- tibble(dataset_id = "dataset1",
                  dataset_type = "array",
                  sample_set_id = "all")
```

### Array data

Metadata describing the array is stored in the array_dataset table.

```{r}
array_dataset <- tibble(
    dataset_id = "dataset1",
    genotyping_center = "Wellcome Trust Sanger Institute",
    array_manufacturer = "Illumina",
    array_name = "Human 1M",
    genotype_calling_software = "BeadStudio",
    reference_assembly = "GRCh37"
)
```

Files are linked to datasets. The md5 hash of each file is used as the primary key for the 'file' table. This should be computed before uploading to the workspace and checked after upload to make sure the upload was successful.

```{r}
files <- paste0(bucket, "/", prefix, c(".bed", ".bim", ".fam"))
md5 <- c("ec6096edea0d6f46191a0275577b3f02",
         "5a1e4276783afa0a235f907edae1dae3",
         "4d9651bb9e45054dc8ed8c1c59cba19d")
file <- tibble(md5sum = md5,
               dataset_id = "dataset1",
               file_path = files,
               description = c("bed", "bim", "fam"))
```


## Write tables as files to workspace bucket

To test the tables using a workflow, they must be written as files to the workspace bucket.

```{r}
table_names <- c("subject", "sample", "sample_set", "dataset", "array_dataset", "file")
for (t in table_names) {
  outfile <- paste0("HapMap_", t, "_table.tsv")
  write_tsv(get(t), outfile)
  gsutil_cp(outfile, bucket)
}
```


## Check tables against data model

Once all tables have been created, we can check that they conform to the data model. This is most easily accomplished by providing the paths to the tables in TSV format as input to the `data_model_report` workflow. Below we illustrate some of the steps in that workflow.

```{r}
primed_tsv <- "https://raw.githubusercontent.com/UW-GAC/primed_data_models/main/PRIMED_data_model_draft.tsv"
model <- tsv_to_dm(list(url(primed_tsv)))
model
```

```{r}
tables <- list(subject=subject,
               sample=sample,
               sample_set=sample_set,
               dataset=dataset,
               array_dataset=array_dataset,
               file=file)
```

```{r}
check_table_names(tables, model)
```

```{r}
check_column_names(tables, model)
```

```{r}
check_column_types(tables, model)
```

```{r}
check_primary_keys(tables, model)
```

```{r}
check_foreign_keys(tables, model)
```

## Add data tables to workspace

Once we have verified that all our tables comply with the data model, we can import them to the workspace. The data model is a required argument to the import functions, as it ensures that the primary keys (also known as 'entity ids' in AnVIL) are created correctly. The `overwrite` argument controls whether to overwrite existing data tables in the workspace.

```{r}
anvil_import_table(subject, table_name="subject", model, overwrite=TRUE)
anvil_import_table(sample, table_name="sample", model, overwrite=TRUE)
```

Importing the sample_set table requires a dedicated function, as AnVIL creates a link between the original table and the set table.

```{r}
anvil_import_set(sample_set, table_name="sample_set", overwrite=TRUE)
```

```{r}
anvil_import_table(dataset, table_name="dataset", model, overwrite=TRUE)
anvil_import_table(array, table_name="array_dataset", model, overwrite=TRUE)
anvil_import_table(file, table_name="file", model, overwrite=TRUE)
```

