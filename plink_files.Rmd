---
title: "PRIMED example notebook: PLINK files"
output: html_notebook
---

# Import data to workspace: PLINK example

We have uploaded HapMap data in PLINK format (bed/bim/fam) to this workspace. In this notebook, we prepare the data from these files for importing into data tables.

## Install and load R packages

```{r}
# get the latest version of AnVIL from github
#remotes::install_github("Bioconductor/AnVIL", dependencies=FALSE)
#remotes::install_github("UW-GAC/AnvilDataModels")
library(AnVIL)
library(AnvilDataModels)
library(tidyverse)
```

## Files in workspace

To access files in this workspace, we need the google bucket ID, which is returned by the `avbucket` function. `gsutil_ls(bucket)` would list all the files in the bucket.

```{r}
(bucket <- avbucket())
```

To read from a google bucket, we use the `gsutil_pipe` function. We need to specify `"rb"` as "read from binary". An alternative is to copy the file to the local instance with `gsutil_cp`.

```{r}
prefix <- "hapmap3_r3_b37_fwd.consensus.qc.poly_Ilmn1M"
famfile <- paste0(bucket, "/", prefix, ".fam")
famfile %in% gsutil_ls(bucket)
fampipe <- gsutil_pipe(famfile, "rb")
fam <- read_table(fampipe, col_names=c("family", "indiv", "father", "mother", "sex", "phen"), col_types="cccccc")
head(fam)
```

## Prepare tables according to data model

### Sample and subject tables

Create subject table

```{r}
subject <- fam %>%
    mutate(reported_sex=c("1"="M", "2"="F")[sex]) %>%
    select(subject_id=indiv, reported_sex) %>%
    mutate(consent_code="NRUP",
          study_nickname="HapMap",
          dbgap_submission=FALSE)
head(subject)
```

Create sample table. In this example we use the same identifiers for subject and sample, but different values for each are preferred.

```{r}
sample <- fam %>%
    select(sample_id=indiv) %>%
    mutate(subject_id=sample_id,
           tissue_source="cell line")
head(sample)
```


### Sample sets

Define sample set to link to genotype data. We will create two sets, one with all samples (recommended for inclusion in every workspace), and one with 100 samples that we will call "set1".

```{r}
sample_set <- create_set_all(sample, table_name="sample")
sample_set_100 <- tibble(sample_set_id="set1", sample_id=sample$sample_id[1:100])
sample_set <- bind_rows(sample_set, sample_set_100)
head(sample_set)
tail(sample_set)
count(sample_set, sample_set_id)
```

### Datasets

Each dataset is linked to a sample_set, but the same sample set may correspond to multiple datasets (such as array data and imputed data).

### Array data

Metadata describing the array is stored in the array_dataset table. We save this as a set of "field" and "value" pairs for input to the workflow that assigns a unique identifier for each dataset.

```{r}
array_fields <- list(
    sample_set_id = "all",
    genotyping_center = "Wellcome Trust Sanger Institute",
    array_manufacturer = "Illumina",
    array_name = "Human 1M",
    genotype_calling_software = "BeadStudio",
    reference_assembly = "GRCh37"
)
array_dataset <- tibble(field=names(array_fields),
                        value=unlist(array_fields))
```

Files are linked to datasets. The md5 hash of each file is used to generate the primary key for the 'file' table. The md5 should be computed before uploading to the workspace and checked after upload to make sure the upload was successful. 

```{r}
files <- paste0(bucket, "/", prefix, c(".bed", ".bim", ".fam"))
md5 <- c("ec6096edea0d6f46191a0275577b3f02",
         "5a1e4276783afa0a235f907edae1dae3",
         "4d9651bb9e45054dc8ed8c1c59cba19d")
array_file <- tibble(md5sum = md5,
                     file_path = files,
                     file_type = c("PLINK bed", "PLINK bim", "PLINK fam"))
```


## Write tables as files to workspace bucket

To check the tables using a workflow, they must be written as files to the workspace bucket.

```{r}
table_names <- c("subject", "sample", "sample_set", "array_dataset", "array_file")
for (t in table_names) {
  outfile <- paste0("HapMap_", t, "_table.tsv")
  write_tsv(get(t), outfile)
  gsutil_cp(outfile, bucket)
}
```


## Check tables against data model

Once all tables have been created, we can check that they conform to the data model. This is most easily accomplished by providing the paths to the tables in TSV format as input to the `genotype_report` workflow.
