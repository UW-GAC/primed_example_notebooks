---
title: "PRIMED example notebook: import from Terra"
output: html_notebook
---

# Import data to workspace: Terra example

In this notebook we will prepare 1000 Genomes data for importing into data tables, using the Bioconductor AnVIL package to interact with the workspace.

## Install and load R packages

```{r}
# get the latest version of AnvilDataModels from github
#remotes::install_github("UW-GAC/AnvilDataModels", upgrade=FALSE)
library(AnVIL)
library(AnvilDataModels)
library(tidyverse)
```

## Source data

We are going to import data from another (open-access) AnVIL workspace.

First, we look at the data tables:

```{r}
avtables(namespace="amp-t2d-op", name="2019_ASHG_Reproducible_GWAS-V2")
```

We import the "sample" table as a tibble:

```{r}
sample_orig <- avtable("sample", namespace="amp-t2d-op", name="2019_ASHG_Reproducible_GWAS-V2")
head(sample_orig)
```

## Prepare tables according to data model

### Sample and subject tables

All phenotypes were simulated for the purposes of the GWAS tutorial in the origin workspace. In PRIMED (as in dbGaP and other AnVIL consortia) we distinguish between subjects (participants in a study) and samples (e.g. blood sample for genotyping). Multiple samples may be associated with the same subject. All of the data in this table more appropriately belongs in the subject table. In the PRIMED data model, we will actually have multiple tables, with the subject table containing key information like study and consent, and separate phenotype tables for each phenotype domain. Here we add required columns for the subject table.

```{r}
subject <- sample_orig %>%
  mutate(reported_sex=c("M"="Male", "F"="Female")[sex]) %>%
  select(subject_id=sample_id,
         reported_sex) %>%
  mutate(consent_code="NRES",
         study_nickname="1000G")
head(subject)
```

The sample table links samples to subjects. For this 1000 Genomes example, sample_id and subject_id have the same values, but using different identifiers is recommended.

```{r}
sample <- sample_orig %>%
  select(sample_id) %>%
  mutate(subject_id=sample_id,
         tissue_source=NA)
head(sample)
```

### Population descriptors

We prepare a data file with population descriptors, which will be listed in the population_descriptors_dataset table. Country of recruitment and country of birth should be included if known. A data dictionary should be included.

```{r}
country_map <- 
  c(ACB="Barbados", ASW="USA", BEB="Bangladesh", CDX="China", CEU="USA", 
    CHB="China", CHS="China", CLM="Columbia", ESN="Nigeria", FIN="Finland", 
    GBR="UK", GIH="USA", GWD="Gambia", IBS="Spain", ITU="UK", JPT="Japan", 
    KHV="Vietnam", LWK="Kenya", MSL="Sierra Leone", MXL="USA", PEL="Peru", 
    PJL="Pakistan", PUR="USA", STU="UK", TSI="Italy", YRI="Nigeria")

pop_desc <- sample_orig %>%
  select(subject_id=sample_id,
         population,
         superpopulation=ancestry) %>%
  mutate(country_of_recruitment=country_map[population])
head(pop_desc)

dd <- tibble(var_name=names(pop_desc),
             description=c("subject identifier",
                           "population",
                           "superpopulation",
                           "country of recruitment"))

pop_desc_file <- "1000G_populations.txt"
pop_desc_dd_file <- "1000G_populations_DD.txt"
write_tsv(pop_desc, pop_desc_file)
write_tsv(dd, pop_desc_dd_file)
```

Copy the files to the workspace bucket and create the population_descriptors_dataset table.

```{r}
bucket <- avbucket()
gsutil_cp(pop_desc_file, bucket)
gsutil_cp(pop_desc_dd_file, bucket)
```


```{r}
population_descriptors_dataset <- tibble(
  study_nickname = "1000G",
  pop_desc_data_file_path = file.path(bucket, pop_desc_file),
  pop_desc_dd_file_path = file.path(bucket, pop_desc_dd_file)
)
population_descriptors_dataset
```

Now we reformat the population descriptors for the standardized population_descriptors table. Multiple descriptors are collapsed into a delimited list for each subject.

```{r}
population_descriptors <- pop_desc %>%
  mutate(population_descriptor = "population | superpopulation",
         population_label = paste(population, superpopulation, sep=" | ")) %>%
  select(subject_id, population_descriptor, population_label, country_of_recruitment)
head(population_descriptors)
```

### Phenotypes

We create a pilot phenotype table using the simulated phenotypes from the original workspace.

```{r}
pheno <- sample_orig %>%
  select(subject_id=sample_id,
         age_at_observation=age,
         height, bmi, ldl)
head(pheno)
```

### Sample sets

Sample sets define groups of samples and link them to datasets. We will create one set with all samples.

```{r}
sample_set <- create_set_all(sample, table_name="sample")
head(sample_set)
```

### Datasets

Each dataset is linked to a sample set. Each type of dataset (array, imputation, sequencing) has its own type-specific columns, which should be saved as a set of field/value pairs. In this case, we have sequencing data.

```{r}
sequencing_fields <- list(
  sample_set_id = "all",
  seq_center = NA,
  reference_assembly = "GRCh37",
  alignment_method = NA,
  sequencing_assay = "WGS",
  seq_platform = "Illumina"
)
sequencing_dataset <- tibble(field=names(sequencing_fields),
                             value=unlist(sequencing_fields))
```


Files are linked to datasets. Rather than copying data, we create a 'sequencing_file' table that references VCF files in their origin workspace.
In the source workspace, the paths to the VCF files are stored in a "Workspace data" table.

```{r}
avdata(namespace="amp-t2d-op", name="2019_ASHG_Reproducible_GWAS-V2")
```

```{r}
vcf <- avdata(namespace="amp-t2d-op", name="2019_ASHG_Reproducible_GWAS-V2") %>%
  filter(key == "vcf_files") %>%
  select(value) %>%
  unlist()
head(vcf)
```

The file paths are in a comma-delimited string, so we extract them into a vector.

```{r}
vf <- strsplit(vcf, split='\",\"', fixed=TRUE) %>%
  unlist(use.names=FALSE)

vf <- sub('[\"', '', vf, fixed=TRUE)
vf <- sub('\"]', '', vf, fixed=TRUE)

vf
```

The primary key of the 'file' table in the PRIMED data model is the md5 hash. As this is not provided by the source workspace, we compute it. We use 'gsutil_pipe' from the AnVIL package to open a connection to the file without copying it to the local instance.

```{r}
md5 <- sapply(vf, function(f) {
  gsutil_pipe(f, "rb") %>%
    openssl::md5() %>%
    as.character()
}, USE.NAMES=FALSE)
```


Extract chromosome from the file name to add to the table.

```{r}
chr <- stringr::str_extract(vf, "chr[:alnum:]+[:punct:]")
chr <- sub("chr", "", chr, fixed=TRUE)
chr <- sub(".", "", chr, fixed=TRUE)
```

```{r}
sequencing_file <- tibble(md5sum = md5,
                          chromosome = chr, 
                          file_path = vf,
                          file_type = "VCF")
```


## Write tables as files to workspace bucket

To check the tables using a workflow, they must be written as files to the workspace bucket.

```{r}
bucket <- avbucket()

table_names <- c("subject", "population_descriptors", 
                 "population_descriptors_dataset",
                 "pheno", "sample", "sample_set", 
                 "sequencing_dataset", "sequencing_file")
for (t in table_names) {
  outfile <- paste0("1000G_", t, "_table.tsv")
  write_tsv(get(t), outfile)
  gsutil_cp(outfile, bucket)
}
```

## Check tables against data model

Once all tables have been created, we can check that they conform to the data model. This is most easily accomplished by providing the paths to the tables in TSV format as input to the `validate_genotype_model`
and `validate_phenotype_model` workflows.
