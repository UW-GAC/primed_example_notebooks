---
title: "Creating a sequencing_sample_file table"
output: html_notebook
---

This notebook illustrates how to create a sequencing_sample_file table in the PRIMED data model.

First, we load needed R libraries.

```{r}
#BiocManager::install("AnVILGCP")
library(AnVIL)
library(dplyr)
library(readr)
library(tidyr)
```

We are using high-coverage 1000 Genomes data as an example, so we read in the necessary data tables from the public AnVIL workspace.

```{r}
workspace <- "1000G-high-coverage-2019"
namespace <- "anvil-datastorage"
participant <- avtable("participant", name=workspace, namespace=namespace)
sample_ws <- avtable("sample", name=workspace, namespace=namespace)
pedigree <- avtable("pedigree", name=workspace, namespace=namespace)
```

The subject and sample tables are required, so we prepare those first.

## Subject table

This table lists subjects with the ID that will link them to phenotypes. In this case, the ID we want is in the SAMPLE_NAME column of the participant table. We also join the pedigree table in order to include reported_sex.

```{r}
subject <- participant %>%
  left_join(pedigree, by=c(SAMPLE_NAME="pedigree_id")) %>%
  mutate(reported_sex = c("1"="Male", "2"="Female")[Sex]) %>%
  select(subject_id = SAMPLE_NAME, reported_sex) %>%
  mutate(consent_code = "NRES",
         study_nickname = "1000G")
```


## Sample table

The sample table links subject IDs to the sample IDs that are included in the BAM or CRAM headers. tissue_source is also a required column, but in this case it is unknown.

```{r}
sample <- sample_ws %>%
  select(sample_id,
         subject_id = library_name) %>%
  mutate(tissue_source="Unknown")
```


## Sequencing sample file table

This table lists BAM, CRAM, VCF or gVCF files associated with a single sample. In this case, the cram and index file paths are in the sample table, but the md5sum (a required field in the PRIMED data model) is in the participant table.


```{r}
sequencing_sample_file <- sample_ws %>%
  left_join(participant, by=c("sample_id"="participant_id")) %>%
  select(sample_id,
         file_path = cram,
         md5sum = MD5SUM) %>%
  mutate(file_type = "CRAM")
```


If additional single-sample files are available (gVCF or single-sample VCF), they may be included in this table as well.

```{r}
gvcf <- sample_ws %>%
  select(sample_id, gVCF, `gVCF index` = gVCF_TBI) %>%
  pivot_longer(starts_with("gVCF"), names_to="file_type", values_to="file_path")
```

Since md5sums are not provided for these files, we will need to retrieve them from the google cloud metadata. We select only the first five rows as an example so the code runs faster.
 
```{r}
md5 <- function(f) {
  AnVILGCP::gsutil_stat(f) %>%
    select(`Hash (md5)`) %>%
    unlist() %>%
    writeLines("md5_b64.txt")
  system("python3 -c \"import base64; import binascii; print(binascii.hexlify(base64.urlsafe_b64decode(open('md5_b64.txt').read())))\" | cut -d \"'\" -f 2 > md5_hex.txt")
  hex <- readLines("md5_hex.txt")
  file.remove(c("md5_hex.txt", "md5_b64.txt"))
  return(hex)
}

gvcf <- gvcf[1:5,] %>%
  rowwise() %>%
  mutate(md5sum = md5(file_path))
```
 
```{r}
sequencing_sample_file <- sequencing_sample_file %>%
  bind_rows(gvcf)
```

 
## Write tables as files to workspace bucket

To check the tables using a workflow, they must be written as files to the workspace bucket.

```{r}
bucket <- avstorage()

table_names <- c("subject",
                 "sample",
                 "sequencing_sample_file")
for (t in table_names) {
  outfile <- paste0("1000G_", t, "_table.tsv")
  write_tsv(get(t), outfile)
  avcopy(outfile, bucket)
}
```

## Check tables against data model

Once all tables have been created, we can check that they conform to the data model and that the md5sums match the files in cloud storage. This is most easily accomplished by providing the paths to the tables in TSV format as input to the `validate_genotype_model` workflow.
